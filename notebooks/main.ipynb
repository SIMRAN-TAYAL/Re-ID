{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33cd048e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\envs\\Vehicle_RE_ID\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "# Add the project root directory to sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "print(project_root)  # Just to verify path added\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a2e1f07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x0000029A35130220>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\envs\\Vehicle_RE_ID\\Re_ID\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"d:\\envs\\Vehicle_RE_ID\\Re_ID\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1562, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    }
   ],
   "source": [
    "from src.dataset import VeRiDataset\n",
    "from src.sampler import RandomIdentitySampler\n",
    "from src.model import ReIDModel\n",
    "from src.losses import TripletLoss\n",
    "import os\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "476e375a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\envs\\Vehicle_RE_ID\\notebooks\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5059d339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingImages\n",
      "37746\n",
      "TestingImages\n",
      "11579\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "\n",
    "with open(\"D:/envs/Vehicle_RE_ID/data/VeRi/train_label.xml\", \"rb\") as f:\n",
    "    xml_bytes = f.read()\n",
    "\n",
    "with open(\"D:/envs/Vehicle_RE_ID/data/VeRi/test_label.xml\", \"rb\") as g:\n",
    "    xml_bytes_ = g.read()\n",
    "\n",
    "xml_text = xml_bytes.decode(\"gb2312\", errors=\"ignore\")\n",
    "root = etree.fromstring(xml_text.encode(\"utf-8\"))\n",
    "\n",
    "xml_text_ = xml_bytes_.decode(\"gb2312\", errors=\"ignore\")\n",
    "root_ = etree.fromstring(xml_text_.encode(\"utf-8\"))\n",
    "\n",
    "print(root.tag)\n",
    "print(len(root.findall(\".//Item\")))\n",
    "\n",
    "print(\"TestingImages\")\n",
    "print(len(root_.findall(\".//Item\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b74c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.Pad(10),\n",
    "    transforms.RandomCrop((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),      # resize to same as training\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2992ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded VeRi split: train\n",
      "  Total images: 37746\n",
      "  Unique vehicle IDs: 575\n",
      "  Cameras: 20\n",
      "  Vehicle ID range (remapped): 0 → 574\n",
      "\n",
      "37746\n",
      "\n",
      "Loaded VeRi split: test\n",
      "  Total images: 11579\n",
      "  Unique vehicle IDs: 200\n",
      "  Cameras: 19\n",
      "  Vehicle ID range (remapped): 0 → 199\n",
      "\n",
      "11579\n"
     ]
    }
   ],
   "source": [
    "dataset_root = 'D:/envs/Vehicle_RE_ID/data/VeRi/'\n",
    "\n",
    "train_dataset = VeRiDataset(dataset_root, split=\"train\", transform=transform_train)\n",
    "print(len(train_dataset))\n",
    "\n",
    "test_dataset = VeRiDataset(dataset_root, split=\"test\", transform=transform_train)\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86d23ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.6906,  0.6906,  0.6734,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [ 0.6906,  0.6906,  0.6734,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [ 0.6906,  0.6906,  0.6734,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          ...,\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "         [[ 0.8529,  0.8529,  0.8354,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [ 0.8529,  0.8529,  0.8354,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [ 0.8529,  0.8529,  0.8179,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          ...,\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "         [[ 0.9668,  0.9668,  0.9668,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [ 0.9842,  0.9842,  0.9668,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [ 0.9842,  0.9842,  1.0017,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          ...,\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]),\n",
       " 14,\n",
       " 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f9a1f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2.1179, -2.1179, -1.3473,  ...,  0.2453,  0.2282,  0.2282],\n",
       "          [-2.1179, -2.1179, -1.3302,  ...,  0.2453,  0.2282,  0.2282],\n",
       "          [-2.1179, -2.1179, -1.3130,  ...,  0.2453,  0.2282,  0.2282],\n",
       "          ...,\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "         [[-2.0357, -2.0357, -1.2304,  ...,  0.3452,  0.3452,  0.3452],\n",
       "          [-2.0357, -2.0357, -1.2129,  ...,  0.3452,  0.3452,  0.3452],\n",
       "          [-2.0357, -2.0357, -1.1954,  ...,  0.3452,  0.3452,  0.3452],\n",
       "          ...,\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "         [[-1.8044, -1.8044, -0.9330,  ...,  0.7576,  0.7228,  0.7054],\n",
       "          [-1.8044, -1.8044, -0.9156,  ...,  0.7576,  0.7228,  0.7054],\n",
       "          [-1.8044, -1.8044, -0.8981,  ...,  0.7576,  0.7228,  0.7054],\n",
       "          ...,\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]),\n",
       " 1,\n",
       " 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fb925f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ For query set — filter from test samples\n",
    "query_list_path = os.path.join(dataset_root, \"name_query.txt\")\n",
    "with open(query_list_path, \"r\") as f:\n",
    "    query_names = [line.strip() for line in f.readlines()]\n",
    "\n",
    "query_samples = [s for s in test_dataset.samples if os.path.basename(s[0]) in query_names]\n",
    "gallery_samples = [s for s in test_dataset.samples if os.path.basename(s[0]) not in query_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75c32234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset instances manually using the same transform\n",
    "query_dataset = torch.utils.data.Subset(test_dataset, range(len(query_samples)))\n",
    "query_dataset.samples = query_samples\n",
    "\n",
    "gallery_dataset = torch.utils.data.Subset(test_dataset, range(len(gallery_samples)))\n",
    "gallery_dataset.samples = gallery_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69053adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training samples: 37746\n",
      "Unique vehicle IDs: 575\n",
      "num_pids_per_batch = 8\n",
      "\n",
      "Test samples: 11579\n",
      "\n",
      "Query samples: 1678\n",
      "\n",
      "Gallery samples: 9901\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Training samples:\", len(train_dataset))\n",
    "unique_vids = len(set([vid for _, vid, _ in train_dataset.samples]))\n",
    "print(\"Unique vehicle IDs:\", unique_vids)\n",
    "\n",
    "batch_size = 32\n",
    "num_instances = 4\n",
    "P = batch_size // num_instances\n",
    "print(f\"num_pids_per_batch = {P}\")\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\nTest samples:\", len(test_dataset))\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\nQuery samples:\", len(query_dataset))\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\nGallery samples:\", len(gallery_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8007f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_instances = 4  # K\n",
    "sampler = RandomIdentitySampler(train_dataset, batch_size, num_instances)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=4, drop_last=True)\n",
    "\n",
    "query_loader = DataLoader(query_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "gallery_loader = DataLoader(gallery_dataset, batch_size=32, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa0caceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 37746\n",
      "Query samples: 1678\n",
      "Gallery samples: 9901\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Query samples: {len(query_dataset)}\")\n",
    "print(f\"Gallery samples: {len(gallery_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ca4eedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch PIDs: tensor([103, 103, 103, 103,  58,  58,  58,  58, 378, 378, 378, 378, 375, 375,\n",
      "        375, 375, 354, 354, 354, 354, 319, 319, 319, 319, 245, 245, 245, 245,\n",
      "        535, 535, 535, 535])\n"
     ]
    }
   ],
   "source": [
    "for imgs, pids, camids in train_loader:\n",
    "    print(\"Batch PIDs:\", pids)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c6756e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch PIDs: tensor([569, 569, 569, 569, 280, 280, 280, 280, 410, 410, 410, 410, 563, 563,\n",
      "        563, 563, 419, 419, 419, 419,   7,   7,   7,   7, 337, 337, 337, 337,\n",
      "        530, 530, 530, 530])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(\"Batch PIDs:\", batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a342f084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e1979a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575\n"
     ]
    }
   ],
   "source": [
    "num_classes=len(set([v for _, v, _ in train_dataset.samples]))\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa721082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to C:\\Users\\dasar/.cache\\torch\\hub\\checkpoints\\resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [00:11<00:00, 8.99MB/s]\n"
     ]
    }
   ],
   "source": [
    "#Create Optimizer and Scheduler\n",
    "\n",
    "model = ReIDModel(num_classes=len(set([v for _, v, _ in train_dataset.samples]))).cuda()\n",
    "\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "criterion_tri = TripletLoss(margin=0.3)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7eec578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReIDModel(\n",
      "  (base): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gap): AdaptiveAvgPool2d(output_size=1)\n",
      "  (embedding): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (classifier): Linear(in_features=512, out_features=575, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96b8a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, ce_loss, tri_loss):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for imgs, pids, camids in dataloader:\n",
    "        imgs = imgs.cuda()\n",
    "        pids = pids.cuda()\n",
    "\n",
    "        cls_score, features = model(imgs)\n",
    "        loss = ce_loss(cls_score, pids) + tri_loss(features, pids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24df3b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 575\n",
      "Min VID: 0, Max VID: 574\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of classes: {unique_vids}\")\n",
    "all_vids = [vid for _, vid, _ in train_dataset.samples]\n",
    "print(f\"Min VID: {min(all_vids)}, Max VID: {max(all_vids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7eabe97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  575 unique IDs\n",
      "Min ID: 0 Max ID: 574\n"
     ]
    }
   ],
   "source": [
    "train_vids = set([vid for _, vid, _ in train_dataset.samples])\n",
    "print(\"Train: \", len(train_vids), \"unique IDs\")\n",
    "print(\"Min ID:\", min(train_vids), \"Max ID:\", max(train_vids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e37d7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  200 unique IDs\n",
      "Min ID: 0 Max ID: 199\n"
     ]
    }
   ],
   "source": [
    "test_vids = set([vid for _, vid, _ in test_dataset.samples])\n",
    "print(\"Test: \", len(test_vids), \"unique IDs\")\n",
    "print(\"Min ID:\", min(test_vids), \"Max ID:\", max(test_vids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1dc12e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/60], Loss: 0.5587\n",
      "Epoch [2/60], Loss: 0.4869\n",
      "Epoch [3/60], Loss: 0.4381\n",
      "Epoch [4/60], Loss: 0.4108\n",
      "Epoch [5/60], Loss: 0.3784\n",
      "Epoch [6/60], Loss: 0.3494\n",
      "Epoch [7/60], Loss: 0.3276\n",
      "Epoch [8/60], Loss: 0.3125\n",
      "Epoch [9/60], Loss: 0.2993\n",
      "Epoch [10/60], Loss: 0.2829\n",
      "Epoch [11/60], Loss: 0.2648\n",
      "Epoch [12/60], Loss: 0.2485\n",
      "Epoch [13/60], Loss: 0.2376\n",
      "Epoch [14/60], Loss: 0.2055\n",
      "Epoch [15/60], Loss: 0.2030\n",
      "Epoch [16/60], Loss: 0.1899\n",
      "Epoch [17/60], Loss: 0.1796\n",
      "Epoch [18/60], Loss: 0.1562\n",
      "Epoch [19/60], Loss: 0.1552\n",
      "Epoch [20/60], Loss: 0.1434\n",
      "Epoch [21/60], Loss: 0.1147\n",
      "Epoch [22/60], Loss: 0.1064\n",
      "Epoch [23/60], Loss: 0.1068\n",
      "Epoch [24/60], Loss: 0.1033\n",
      "Epoch [25/60], Loss: 0.1007\n",
      "Epoch [26/60], Loss: 0.1004\n",
      "Epoch [27/60], Loss: 0.0937\n",
      "Epoch [28/60], Loss: 0.0923\n",
      "Epoch [29/60], Loss: 0.0861\n",
      "Epoch [30/60], Loss: 0.0855\n",
      "Epoch [31/60], Loss: 0.0864\n",
      "Epoch [32/60], Loss: 0.0863\n",
      "Epoch [33/60], Loss: 0.0847\n",
      "Epoch [34/60], Loss: 0.0829\n",
      "Epoch [35/60], Loss: 0.0823\n",
      "Epoch [36/60], Loss: 0.0824\n",
      "Epoch [37/60], Loss: 0.0767\n",
      "Epoch [38/60], Loss: 0.0759\n",
      "Epoch [39/60], Loss: 0.0736\n",
      "Epoch [40/60], Loss: 0.0719\n",
      "Epoch [41/60], Loss: 0.0719\n",
      "Epoch [42/60], Loss: 0.0698\n",
      "Epoch [43/60], Loss: 0.0725\n",
      "Epoch [44/60], Loss: 0.0689\n",
      "Epoch [45/60], Loss: 0.0685\n",
      "Epoch [46/60], Loss: 0.0730\n",
      "Epoch [47/60], Loss: 0.0695\n",
      "Epoch [48/60], Loss: 0.0654\n",
      "Epoch [49/60], Loss: 0.0680\n",
      "Epoch [50/60], Loss: 0.0678\n",
      "Epoch [51/60], Loss: 0.0686\n",
      "Epoch [52/60], Loss: 0.0695\n",
      "Epoch [53/60], Loss: 0.0710\n",
      "Epoch [54/60], Loss: 0.0683\n",
      "Epoch [55/60], Loss: 0.0677\n",
      "Epoch [56/60], Loss: 0.0670\n",
      "Epoch [57/60], Loss: 0.0678\n",
      "Epoch [58/60], Loss: 0.0644\n",
      "Epoch [59/60], Loss: 0.0680\n",
      "Epoch [60/60], Loss: 0.0657\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 60\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train_one_epoch(model, train_loader, optimizer, criterion_ce, criterion_tri)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f15c6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"D:/envs/Vehicle_RE_ID/Outputs/checkpoints/ReID_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dbc8e0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "pids_list = [np.array([1, 2]), np.array([3, 4, 5])]\n",
    "np.concatenate(pids_list)  # [1,2,3,4,5]\n",
    "print(type(pids_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad2f618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== mAP-only evaluation ======\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1) Extract embeddings for a dataloader (returns numpy arrays)\n",
    "def extract_embeddings(model, loader, device, cache_path):\n",
    "    \"\"\"\n",
    "    Extracts embeddings for all samples from `loader`.\n",
    "    Returns:\n",
    "        feats: numpy array shape (N, D)\n",
    "        pids: numpy array shape (N,)\n",
    "        camids: numpy array shape (N,)\n",
    "    \"\"\"\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"⚡ Loading cached features from {cache_path}\")\n",
    "        data = np.load(cache_path, allow_pickle=True)\n",
    "        return data[\"feats\"], data[\"pids\"], data[\"camids\"]\n",
    "\n",
    "    model.eval()\n",
    "    feats_list = []\n",
    "    pids_list = []\n",
    "    camids_list = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, pids, camids in tqdm(loader, desc=\"Extract feats\", ncols=120):\n",
    "            imgs = imgs.to(device)\n",
    "            # assume model can return feature vector when asked\n",
    "            feat = model(imgs, return_feature=True)  # returns tensor [B, D]\n",
    "            feat = feat.detach().cpu().numpy()\n",
    "            feats_list.append(feat)\n",
    "            pids_list.append(np.asarray(pids))\n",
    "            camids_list.append(np.asarray(camids))\n",
    "    feats = np.vstack(feats_list)\n",
    "    pids = np.concatenate(pids_list)\n",
    "    camids = np.concatenate(camids_list)\n",
    "\n",
    "    np.savez(cache_path, feats=feats, pids=pids, camids=camids)\n",
    "    print(f\"✅ Features cached at {cache_path}\")\n",
    "\n",
    "    return feats, pids, camids\n",
    "\n",
    "# 2) Pairwise Euclidean distance (query vs gallery)\n",
    "def compute_euclidean_distance(q_feats, g_feats):\n",
    "    # Efficient squared Euclidean distance: (a-b)^2 = a^2 + b^2 - 2ab\n",
    "    # returns shape (Q, G)\n",
    "    q2 = np.sum(np.square(q_feats), axis=1, keepdims=True)  # (Q,1)\n",
    "    g2 = np.sum(np.square(g_feats), axis=1, keepdims=True).T  # (1,G)\n",
    "    dist = q2 + g2 - 2.0 * np.dot(q_feats, g_feats.T)\n",
    "    dist = np.maximum(dist, 0.0)\n",
    "    return dist\n",
    "\n",
    "# 3) Compute AP for one query (helper)\n",
    "def average_precision_at_ranks(sorted_matches):\n",
    "    \"\"\"\n",
    "    sorted_matches: binary 1D array (length G) where 1 indicates correct match at that rank.\n",
    "    AP = average of precision@k across ranks where sorted_matches[k]==1\n",
    "    \"\"\"\n",
    "    # cumulative sum of matches gives number of correct up to position k\n",
    "    cum_matches = np.cumsum(sorted_matches)\n",
    "    # positions where match occurs (0-based indices)\n",
    "    match_indices = np.where(sorted_matches == 1)[0]\n",
    "    if match_indices.size == 0:\n",
    "        return 0.0\n",
    "    precisions = [(cum_matches[k] / (k + 1)) for k in match_indices]\n",
    "    ap = np.mean(precisions)\n",
    "    return ap\n",
    "\n",
    "# 4) mAP computation following standard Re-ID protocol\n",
    "def compute_map(q_feats, g_feats, q_pids, g_pids, q_camids, g_camids,\n",
    "                metric=\"euclidean\", exclude_same_camera=True):\n",
    "    \"\"\"\n",
    "    Compute mean Average Precision (mAP)\n",
    "    metric: 'euclidean' or 'cosine'\n",
    "    exclude_same_camera: if True, skip same PID + same camera pairs\n",
    "    \"\"\"\n",
    "    if metric == \"euclidean\":\n",
    "        distmat = compute_euclidean_distance(q_feats, g_feats)\n",
    "    elif metric == \"cosine\":\n",
    "        sim = cosine_similarity(q_feats, g_feats)\n",
    "        distmat = 1 - sim  # smaller = closer\n",
    "    else:\n",
    "        raise ValueError(\"metric must be 'euclidean' or 'cosine'\")\n",
    "\n",
    "    num_q, num_g = distmat.shape\n",
    "    print(f\"Distance matrix shape: {distmat.shape}\")\n",
    "\n",
    "    aps, valid_q = [], 0\n",
    "\n",
    "    for q_idx in range(num_q):\n",
    "        q_pid = q_pids[q_idx]\n",
    "        q_camid = q_camids[q_idx]\n",
    "\n",
    "        # Sort gallery images by distance ascending\n",
    "        order = np.argsort(distmat[q_idx, :])\n",
    "        matches = (g_pids[order] == q_pid).astype(np.int32)\n",
    "\n",
    "        if exclude_same_camera:\n",
    "            same_cam = (g_pids[order] == q_pid) & (g_camids[order] == q_camid)\n",
    "            keep = np.invert(same_cam)\n",
    "            matches = matches[keep]\n",
    "\n",
    "        if not np.any(matches):\n",
    "            continue  # no valid match → skip\n",
    "\n",
    "        # Compute Average Precision (AP)\n",
    "        index = np.where(matches == 1)[0]\n",
    "        precisions = [(i + 1) / (rank + 1.0) for i, rank in enumerate(index)]\n",
    "        aps.append(np.mean(precisions))\n",
    "        valid_q += 1\n",
    "\n",
    "    if len(aps) == 0:\n",
    "        print(\"⚠️ No valid queries for mAP computation!\")\n",
    "        return 0.0, 0\n",
    "\n",
    "    mean_ap = np.mean(aps)\n",
    "    return mean_ap, valid_q\n",
    "\n",
    "def compute_cmc_map(q_feats, g_feats, q_pids, g_pids, q_camids, g_camids,\n",
    "                    metric=\"euclidean\", exclude_same_camera=True, ranks=(1, 5, 10, 20, 50, 100)):\n",
    "    \"\"\"\n",
    "    Compute both CMC (Rank@K) and mAP for Re-ID evaluation.\n",
    "    Returns:\n",
    "        cmc_curve: numpy array with cumulative match scores for given ranks\n",
    "        mAP: mean Average Precision\n",
    "    \"\"\"\n",
    "    # ---- Distance computation ----\n",
    "    if metric == \"euclidean\":\n",
    "        distmat = compute_euclidean_distance(q_feats, g_feats)\n",
    "    elif metric == \"cosine\":\n",
    "        sim = cosine_similarity(q_feats, g_feats)\n",
    "        distmat = 1 - sim\n",
    "    else:\n",
    "        raise ValueError(\"metric must be 'euclidean' or 'cosine'\")\n",
    "\n",
    "    num_q, num_g = distmat.shape\n",
    "    cmc = np.zeros(num_g)\n",
    "    aps = []\n",
    "    valid_q = 0\n",
    "\n",
    "    for q_idx in range(num_q):\n",
    "        q_pid = q_pids[q_idx]\n",
    "        q_camid = q_camids[q_idx]\n",
    "\n",
    "        order = np.argsort(distmat[q_idx, :])  # ascending order (closest first)\n",
    "        matches = (g_pids[order] == q_pid).astype(np.int32)\n",
    "\n",
    "        # remove same-camera same-pid images\n",
    "        if exclude_same_camera:\n",
    "            keep = ~((g_pids[order] == q_pid) & (g_camids[order] == q_camid))\n",
    "            matches = matches[keep]\n",
    "\n",
    "        if not np.any(matches):\n",
    "            continue\n",
    "\n",
    "        # ---- CMC computation ----\n",
    "        first_match_idx = np.where(matches == 1)[0][0]\n",
    "        cmc[first_match_idx:] += 1  # everything beyond first correct counts as correct at that rank\n",
    "\n",
    "        # ---- mAP computation ----\n",
    "        index = np.where(matches == 1)[0]\n",
    "        precisions = [(i + 1) / (rank + 1.0) for i, rank in enumerate(index)]\n",
    "        aps.append(np.mean(precisions))\n",
    "        valid_q += 1\n",
    "\n",
    "    if valid_q == 0:\n",
    "        raise RuntimeError(\"No valid queries found!\")\n",
    "\n",
    "    cmc = cmc / valid_q\n",
    "    mAP = np.mean(aps)\n",
    "\n",
    "    # ---- Report Rank@K ----\n",
    "    print(\"\\n=== Rank@K results ===\")\n",
    "    for k in ranks:\n",
    "        if k - 1 < len(cmc):\n",
    "            print(f\"Rank@{k:<3}: {cmc[k-1]*100:.2f}%\")\n",
    "    print(f\"mAP: {mAP:.4f} (valid queries: {valid_q}/{len(q_pids)})\")\n",
    "\n",
    "    return cmc, mAP\n",
    "\n",
    "def plot_cmc_curve(cmc, ranks=(1, 5, 10, 20, 50, 100)):\n",
    "    \"\"\"\n",
    "    Simple CMC curve plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(np.arange(1, len(cmc)+1), cmc * 100, marker='o')\n",
    "    plt.title(\"CMC Curve\")\n",
    "    plt.xlabel(\"Rank\")\n",
    "    plt.ylabel(\"Matching Accuracy (%)\")\n",
    "    plt.grid(True)\n",
    "    plt.xlim(1, max(ranks))\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_map(model, query_loader, gallery_loader, cache_dir):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    query_emds_path = f'{cache_dir}query_feats.npz'\n",
    "    gallery_emds_path = f'{cache_dir}gallery_feats.npz'\n",
    "\n",
    "    q_feats, q_pids, q_camids = extract_embeddings(model, query_loader, device, query_emds_path)\n",
    "    g_feats, g_pids, g_camids = extract_embeddings(model, gallery_loader, device, gallery_emds_path)\n",
    "\n",
    "    # Normalize embeddings\n",
    "    q_feats = q_feats / (np.linalg.norm(q_feats, axis=1, keepdims=True) + 1e-12)\n",
    "    g_feats = g_feats / (np.linalg.norm(g_feats, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "    # Euclidean\n",
    "    print(\"\\n--- Computing mAP (Euclidean, exclude same-camera=True) ---\")\n",
    "    map_euclidean, valid_euclidean = compute_map(\n",
    "        q_feats, g_feats, q_pids, g_pids, q_camids, g_camids,\n",
    "        metric=\"euclidean\", exclude_same_camera=True\n",
    "    )\n",
    "    print(f\"✅ mAP (Euclidean): {map_euclidean:.4f} (valid queries: {valid_euclidean}/{len(q_pids)})\")\n",
    "\n",
    "    # Cosine\n",
    "    print(\"\\n--- Computing mAP (Cosine, exclude same-camera=True) ---\")\n",
    "    map_cosine, valid_cosine = compute_map(\n",
    "        q_feats, g_feats, q_pids, g_pids, q_camids, g_camids,\n",
    "        metric=\"cosine\", exclude_same_camera=True\n",
    "    )\n",
    "    print(f\"✅ mAP (Cosine): {map_cosine:.4f} (valid queries: {valid_cosine}/{len(q_pids)})\")\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    cmc_curve, map_val = compute_cmc_map(\n",
    "    q_feats, g_feats,\n",
    "    q_pids, g_pids,\n",
    "    q_camids, g_camids,\n",
    "    metric=\"euclidean\",  # or \"cosine\"\n",
    "    exclude_same_camera=True)\n",
    "\n",
    "    return map_euclidean, map_cosine, cmc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1120811b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/envs/Vehicle_RE_ID/Outputs/cached_embeddings/query_feats.npz\n"
     ]
    }
   ],
   "source": [
    "cache_dir=\"D:/envs/Vehicle_RE_ID/Outputs/cached_embeddings/\"\n",
    "a = os.path.join(cache_dir, \"query_feats.npz\")\n",
    "print(f'{cache_dir}query_feats.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca1bea8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8907,  0.0618, -0.5712,  ...,  0.7153,  0.1205,  1.5095],\n",
      "        [-0.3645,  0.2994, -0.8402,  ...,  0.8733,  0.0723,  1.3386],\n",
      "        [ 0.0457,  1.0034, -0.9233,  ...,  0.4304, -0.3666,  0.9590],\n",
      "        ...,\n",
      "        [ 1.1225,  0.6968, -0.2114,  ...,  0.0707,  0.9335,  1.0055],\n",
      "        [ 0.8876, -1.7711, -0.2330,  ...,  0.4866,  0.8152,  1.5184],\n",
      "        [ 0.7090, -1.6507, -1.1927,  ...,  0.3278,  0.2586,  0.8275]],\n",
      "       device='cuda:0', grad_fn=<NativeBatchNormBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "for IMGS, _, _ in query_loader:\n",
    "    IMGS = IMGS.to(device)\n",
    "\n",
    "    FEAT = model(IMGS, return_feature=True)\n",
    "\n",
    "    print(FEAT)\n",
    "    print(type(FEAT))\n",
    "    print(FEAT.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4e2b300c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Loading cached features from D:/envs/Vehicle_RE_ID/Outputs/cached_embeddings/query_feats.npz\n",
      "⚡ Loading cached features from D:/envs/Vehicle_RE_ID/Outputs/cached_embeddings/gallery_feats.npz\n",
      "\n",
      "--- Computing mAP (Euclidean, exclude same-camera=True) ---\n",
      "Distance matrix shape: (1678, 9901)\n",
      "✅ mAP (Euclidean): 0.6269 (valid queries: 1678/1678)\n",
      "\n",
      "--- Computing mAP (Cosine, exclude same-camera=True) ---\n",
      "Distance matrix shape: (1678, 9901)\n",
      "✅ mAP (Cosine): 0.6269 (valid queries: 1678/1678)\n",
      "\n",
      "=== Rank@K results ===\n",
      "Rank@1  : 87.84%\n",
      "Rank@5  : 95.05%\n",
      "Rank@10 : 97.44%\n",
      "Rank@20 : 98.33%\n",
      "Rank@50 : 99.05%\n",
      "Rank@100: 99.34%\n",
      "mAP: 0.6269 (valid queries: 1678/1678)\n",
      "\n",
      "==============================\n",
      "✅ Final mAP (Euclidean): 0.6269\n",
      "✅ Final mAP (Cosine):    0.6269\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "map_euclidean, map_cosine, cmc = evaluate_map(\n",
    "    model=model,\n",
    "    query_loader=query_loader,\n",
    "    gallery_loader=gallery_loader,\n",
    "    cache_dir=\"D:/envs/Vehicle_RE_ID/Outputs/cached_embeddings/\"  # saves embeddings for reuse\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 4: Print final results\n",
    "# ---------------------------------------------\n",
    "print(\"\\n==============================\")\n",
    "print(f\"✅ Final mAP (Euclidean): {map_euclidean:.4f}\")\n",
    "print(f\"✅ Final mAP (Cosine):    {map_cosine:.4f}\")\n",
    "print(\"==============================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1cbff5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_cmc_curve(cmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5790b95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Re_ID",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
